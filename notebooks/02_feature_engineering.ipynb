{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b869a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from src.data.preprocessors import clean_price_data\n",
    "from src.data.feature_store import FeatureEngineer\n",
    "\n",
    "csv_path = os.path.join('data', 'raw', 'prices.csv')\n",
    "prices = pd.read_csv(csv_path, parse_dates=['date'])\n",
    "prices = clean_price_data(prices)\n",
    "\n",
    "fe = FeatureEngineer({})\n",
    "features = fe.create_features(prices)\n",
    "features = fe.create_target(features)\n",
    "features = features.dropna()\n",
    "print('Engineered rows:', len(features), 'features:', len(features.columns))\n",
    "\n",
    "# Persist engineered dataset\n",
    "out_parquet = os.path.join('data', 'processed', 'features.parquet')\n",
    "try:\n",
    "    features.to_parquet(out_parquet, index=False)\n",
    "    print('Wrote', out_parquet)\n",
    "except Exception as e:\n",
    "    out_csv = os.path.join('data', 'processed', 'features.csv')\n",
    "    features.to_csv(out_csv, index=False)\n",
    "    print('Parquet unavailable, wrote', out_csv, 'Error:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Sanity checks and sorting\n",
    "assert {'symbol','date','close'}.issubset(set(prices.columns))\n",
    "prices = prices.sort_values(['symbol','date']).reset_index(drop=True)\n",
    "print('After sort rows:', len(prices))\n",
    "print(prices.head(3))\n",
    "print(prices.tail(3))\n",
    "\n",
    "# Basic uniqueness check\n",
    "uniq = prices[['symbol','date']].drop_duplicates()\n",
    "print('Unique (symbol,date) pairs:', len(uniq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809a68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Basic derived columns (returns)\n",
    "prices['ret_1'] = prices.groupby('symbol')['close'].pct_change()\n",
    "prices['ret_5'] = prices.groupby('symbol')['close'].pct_change(5)\n",
    "prices['ret_10'] = prices.groupby('symbol')['close'].pct_change(10)\n",
    "print(prices[['symbol','date','close','ret_1','ret_5','ret_10']].head(12))\n",
    "\n",
    "# Clip extreme returns for stability\n",
    "prices['ret_1'] = prices['ret_1'].clip(-0.3, 0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5089b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Use FeatureEngineer to compute advanced features\n",
    "from src.data.feature_store import FeatureEngineer\n",
    "fe = FeatureEngineer({})\n",
    "feat_df = fe.create_features(prices.copy())\n",
    "print('Feature columns added:', len(feat_df.columns) - len(prices.columns))\n",
    "print(feat_df.head(5))\n",
    "\n",
    "# Keep only needed columns for modeling\n",
    "core_cols = ['symbol','date','close']\n",
    "feature_cols = [c for c in feat_df.columns if c not in core_cols]\n",
    "print('Total features:', len(feature_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c05618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Lag features and leakage checks\n",
    "lag_cols = ['close','volume']\n",
    "for c in lag_cols:\n",
    "    for l in [1,2,3,5]:\n",
    "        feat_df[f'{c}_lag_{l}'] = feat_df.groupby('symbol')[c].shift(l)\n",
    "\n",
    "# Ensure target uses only future info later; no label leakage in features\n",
    "print(feat_df[[c for c in feat_df.columns if 'lag' in c]].head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6baaf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Create modeling target\n",
    "horizon = 1\n",
    "threshold = 0.02\n",
    "feat_df = fe.create_target(feat_df, horizon=horizon, threshold=threshold)\n",
    "\n",
    "print(feat_df[['symbol','date','future_return','target']].head(12))\n",
    "print('Target distribution:', feat_df['target'].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebc058b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Handle missing values and final feature selection\n",
    "model_cols = [c for c in feat_df.columns if c not in ['future_return','target_return']]\n",
    "feat_df = feat_df[model_cols]\n",
    "\n",
    "# Drop rows with missing critical inputs\n",
    "feat_df = feat_df.dropna(subset=['target']).copy()\n",
    "feat_df = feat_df.dropna()\n",
    "\n",
    "# Separate metadata and features\n",
    "meta_cols = ['symbol','date','target']\n",
    "X_cols = [c for c in feat_df.columns if c not in meta_cols]\n",
    "print('Final feature count:', len(X_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fee023b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Save feature matrix\n",
    "import os\n",
    "os.makedirs('data/processed', exist_ok=True)\n",
    "\n",
    "out_path = 'data/processed/feature_matrix.parquet'\n",
    "try:\n",
    "    feat_df.to_parquet(out_path, index=False)\n",
    "    print('Saved to', out_path)\n",
    "except Exception as e:\n",
    "    out_csv = 'data/processed/feature_matrix.csv'\n",
    "    feat_df.to_csv(out_csv, index=False)\n",
    "    print('Parquet unavailable; wrote CSV to', out_csv, 'Error:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33541f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Versioned feature store example (SQLite schema)\n",
    "import sqlite3\n",
    "conn = sqlite3.connect('features.db')\n",
    "version = 'v1'\n",
    "\n",
    "# Store a small sample to demonstrate schema (JSON-like via CSV string)\n",
    "sample = feat_df.head(5).copy()\n",
    "sample['feature_version'] = version\n",
    "sample.to_sql('features', conn, if_exists='append', index=False)\n",
    "print('Inserted sample features with version', version)\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4510c1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Feature importance proxy (variance)\n",
    "variances = feat_df[X_cols].var().sort_values(ascending=False)\n",
    "print('Top 20 high variance features:')\n",
    "print(variances.head(20))\n",
    "\n",
    "# Low variance filter example\n",
    "low_var = variances[variances < 1e-8].index.tolist()\n",
    "print('Low variance features (to consider dropping):', low_var[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a96bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Normalization example (z-score)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled = scaler.fit_transform(feat_df[X_cols])\n",
    "scaled_df = pd.DataFrame(scaled, columns=X_cols)\n",
    "scaled_df.insert(0, 'symbol', feat_df['symbol'].values)\n",
    "scaled_df.insert(1, 'date', feat_df['date'].values)\n",
    "scaled_df['target'] = feat_df['target'].values\n",
    "print('Scaled shape:', scaled_df.shape)\n",
    "print(scaled_df.head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6041fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Train/validation split (time-based)\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "ts = TimeSeriesSplit(n_splits=3)\n",
    "X = scaled_df[X_cols]\n",
    "y = scaled_df['target']\n",
    "for fold, (tr, va) in enumerate(ts.split(X)):\n",
    "    print('Fold', fold, 'train:', len(tr), 'val:', len(va))\n",
    "    # Just display a few rows\n",
    "    print('Train head:\\n', scaled_df.iloc[tr].head(2))\n",
    "    print('Val head:\\n', scaled_df.iloc[va].head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1d7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Save scaler + metadata for reproducibility\n",
    "import joblib\n",
    "\n",
    "artifacts_dir = 'data/processed'\n",
    "os.makedirs(artifacts_dir, exist_ok=True)\n",
    "joblib.dump({'scaler': scaler, 'features': X_cols}, os.path.join(artifacts_dir, 'scaler_and_features.joblib'))\n",
    "print('Saved scaler and feature list to data/processed/scaler_and_features.joblib')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
