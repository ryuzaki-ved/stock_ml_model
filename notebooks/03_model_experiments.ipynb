{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa394e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure project root on path\n",
    "ROOT = os.path.abspath('.')\n",
    "if ROOT not in sys.path:\n",
    "    sys.path.insert(0, ROOT)\n",
    "\n",
    "from src.training.trainer import ModelTrainer\n",
    "from src.data.feature_store import FeatureEngineer\n",
    "\n",
    "# Load data (either processed features or raw -> engineer)\n",
    "proc_parquet = os.path.join('data', 'processed', 'features.parquet')\n",
    "raw_csv = os.path.join('data', 'raw', 'prices.csv')\n",
    "\n",
    "if os.path.exists(proc_parquet):\n",
    "    df = pd.read_parquet(proc_parquet)\n",
    "else:\n",
    "    prices = pd.read_csv(raw_csv, parse_dates=['date'])\n",
    "    fe = FeatureEngineer({})\n",
    "    df = fe.create_features(prices)\n",
    "    df = fe.create_target(df)\n",
    "\n",
    "df = df.dropna()\n",
    "config = {\n",
    "    'mlflow_uri': 'sqlite:///mlflow.db',\n",
    "    'experiment_name': 'stock_prediction',\n",
    "    'registered_model_name': 'stock_predictor',\n",
    "    'model_params': {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 3,\n",
    "        'metric': 'multi_logloss',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "    }\n",
    "}\n",
    "\n",
    "trainer = ModelTrainer(config)\n",
    "result = trainer.train(df, model_type='lgbm')\n",
    "print('Trained model with', len(result['feature_cols']), 'features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe915542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Dataset split and feature list\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "df = df.sort_values(['symbol','date']).reset_index(drop=True)\n",
    "feature_cols = [c for c in df.columns if c not in ['symbol','date','target','target_return','future_return']]\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "print('Features:', len(feature_cols), 'Rows:', len(df))\n",
    "print('Feature sample:', feature_cols[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5caafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) TimeSeriesSplit CV\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "ts = TimeSeriesSplit(n_splits=3)\n",
    "folds = []\n",
    "for fold, (tr, va) in enumerate(ts.split(X)):\n",
    "    folds.append({'fold': fold, 'train': len(tr), 'val': len(va)})\n",
    "print(folds)\n",
    "\n",
    "# Show a small preview of indices\n",
    "print('Fold 0 train head:', tr[:5] if len(tr) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bdd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Baseline model (LightGBM via ModelTrainer)\n",
    "from src.training.trainer import ModelTrainer\n",
    "\n",
    "config = {\n",
    "    'mlflow_uri': 'sqlite:///mlflow.db',\n",
    "    'experiment_name': 'stock_prediction',\n",
    "    'registered_model_name': 'stock_predictor',\n",
    "    'model_params': {\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 3,\n",
    "        'metric': 'multi_logloss',\n",
    "        'learning_rate': 0.05,\n",
    "        'num_leaves': 31,\n",
    "    }\n",
    "}\n",
    "trainer = ModelTrainer(config)\n",
    "res = trainer.train(df, model_type='lgbm')\n",
    "print('CV folds:', len(res['cv_scores']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3801759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Evaluate baseline on a holdout (last 10%)\n",
    "cut = int(len(df) * 0.9)\n",
    "X_train, X_test = X.iloc[:cut], X.iloc[cut:]\n",
    "y_train, y_test = y.iloc[:cut], y.iloc[cut:]\n",
    "\n",
    "# Use the trained model from trainer res\n",
    "model = res['model']\n",
    "probs = model.predict(X_test)\n",
    "preds = (probs.argmax(axis=1) - 1)\n",
    "acc = (preds == y_test.values).mean()\n",
    "print('Holdout accuracy:', round(float(acc), 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, preds, labels=[-1,0,1])\n",
    "print('Confusion matrix (rows=true, cols=pred):')\n",
    "print(cm)\n",
    "\n",
    "# Per-class accuracy\n",
    "per_class_acc = (cm.diagonal() / cm.sum(axis=1).clip(min=1)).round(3)\n",
    "print('Per-class accuracy (-1,0,1):', per_class_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a502cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Feature importance (top 20)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "importance = model.feature_importance(importance_type='gain')\n",
    "fi = pd.DataFrame({'feature': res['feature_cols'], 'importance': importance})\n",
    "fi = fi.sort_values('importance', ascending=False).head(20)\n",
    "print(fi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf2cce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Simple probability calibration check\n",
    "# Probability mass should sum ~1 across three classes\n",
    "row0 = probs[0]\n",
    "print('First prob vector:', row0, 'Sum:', float(row0.sum()))\n",
    "\n",
    "# Check distribution of max confidence\n",
    "max_conf = probs.max(axis=1)\n",
    "print('Max confidence stats:', pd.Series(max_conf).describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cb8d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Backtesting integration (optional)\n",
    "from src.training.backtester import Backtester\n",
    "\n",
    "cfg = {\n",
    "    'initial_capital': 100000,\n",
    "    'transaction_cost': 0.0005,\n",
    "    'train_window_days': 60,\n",
    "    'test_window_days': 20,\n",
    "    'retrain_frequency_days': 20,\n",
    "    'confidence_threshold': 0.5\n",
    "}\n",
    "\n",
    "# Use a reduced subset to keep runtime moderate\n",
    "sub = df[['symbol','date','close'] + feature_cols].tail(5000).copy()\n",
    "bt = Backtester(cfg)\n",
    "summary = bt.run(sub, model, feature_cols)\n",
    "print({k: v for k, v in summary.items() if k not in ['daily_values','all_trades']})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be73634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Save model artifacts\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "\n",
    "run = mlflow.last_active_run()\n",
    "if run is not None:\n",
    "    print('Last MLflow run:', run.info.run_id)\n",
    "else:\n",
    "    print('No active MLflow run context available here.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7827eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Simple hyperparameter sweep placeholder\n",
    "# (For a real run, implement in src/training/hyperparameter.py)\n",
    "leaves = [15, 31]\n",
    "lrs = [0.05, 0.1]\n",
    "results = []\n",
    "for nl in leaves:\n",
    "    for lr in lrs:\n",
    "        print('Trying num_leaves=', nl, 'lr=', lr)\n",
    "        # Placeholder: you would instantiate a trainer with these params and evaluate\n",
    "        results.append({'num_leaves': nl, 'learning_rate': lr, 'score': None})\n",
    "print('Sweep grid size:', len(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed17da68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Persist evaluation split for reproducibility\n",
    "out_eval = 'data/processed/holdout_eval.parquet'\n",
    "try:\n",
    "    pd.DataFrame({'y_true': y_test.values, 'y_pred': preds}).to_parquet(out_eval, index=False)\n",
    "    print('Wrote', out_eval)\n",
    "except Exception as e:\n",
    "    out_eval_csv = 'data/processed/holdout_eval.csv'\n",
    "    pd.DataFrame({'y_true': y_test.values, 'y_pred': preds}).to_csv(out_eval_csv, index=False)\n",
    "    print('Parquet unavailable, wrote', out_eval_csv, 'Error:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73082af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Summary\n",
    "print('Training complete.')\n",
    "print('CV scores sample:', res['cv_scores'][:2])\n",
    "print('Holdout accuracy:', round(float(acc), 4))\n",
    "print('Top-5 features by importance:')\n",
    "print(fi.head(5))\n",
    "print('Artifacts saved (if MLflow configured).')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
